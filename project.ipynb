{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Segmentation Analysis\n",
    "\n",
    "## 1. Setup and Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Data Cleaning\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Data Transformation\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Feature selection\n",
    "features = data[['age', 'income', 'spending_score']]  # Example feature columns\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['income'], bins=30, kde=True)\n",
    "plt.title('Income Distribution')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate Analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='age', y='income', data=data)\n",
    "plt.title('Age vs Income')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Income')\n",
    "plt.show()\n",
    "\n",
    "# Multivariate Analysis (PCA for visualization)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_features)\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=pca_df)\n",
    "plt.title('PCA of Customer Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. K-Means Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Fit K-Means with the chosen number of clusters (e.g., 4)\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "clusters = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Add cluster information to the original data\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "# Cluster Analysis\n",
    "print(data.groupby('Cluster').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Compute the distance matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram = sch.dendrogram(sch.linkage(scaled_features, method='ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()\n",
    "\n",
    "# Fit Hierarchical Clustering\n",
    "hc = sch.AgglomerativeClustering(n_clusters=optimal_k, affinity='euclidean', linkage='ward')\n",
    "hc_clusters = hc.fit_predict(scaled_features)\n",
    "\n",
    "# Add cluster information to the original data\n",
    "data['HC_Cluster'] = hc_clusters\n",
    "\n",
    "# Cluster Analysis\n",
    "print(data.groupby('HC_Cluster').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. DBSCAN Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Fit DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_clusters = dbscan.fit_predict(scaled_features)\n",
    "\n",
    "# Add cluster information to the original data\n",
    "data['DBSCAN_Cluster'] = dbscan_clusters\n",
    "\n",
    "# Cluster Analysis\n",
    "print(data.groupby('DBSCAN_Cluster').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Visualization of Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-Means Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=data['Cluster'], palette='viridis')\n",
    "plt.title('K-Means Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Visualize Hierarchical Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=data['HC_Cluster'], palette='viridis')\n",
    "plt.title('Hierarchical Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Visualize DBSCAN Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=data['DBSCAN_Cluster'].astype(str), palette='viridis')\n",
    "plt.title('DBSCAN Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Reporting\n",
    "\n",
    "- **Technical Report:** Document the methodology, results, and interpretations.\n",
    "- **Executive Summary:** Summarize the key findings and actionable insights.\n",
    "- **Dashboards:** Use tools like Tableau or Power BI for interactive visualizations if needed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
